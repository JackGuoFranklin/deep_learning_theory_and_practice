{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import gym\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from collections import defaultdict\n",
    "if \"../\" not in sys.path:\n",
    "  sys.path.append(\"../\") \n",
    "from envs.blackjack import BlackjackEnv\n",
    "from common import plotting\n",
    "\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = BlackjackEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_policy(nA):\n",
    "    \"\"\"\n",
    "    生成一个随机策略函数\n",
    "    \n",
    "    参数:\n",
    "        nA: 所有行为的个数\n",
    "    \n",
    "    返回值:\n",
    "        返回一个函数，它的输入是状态(observation)，输出是长度为nA的numpy数值，表示每个行为的概率\n",
    "    \"\"\"\n",
    "    A = np.ones(nA, dtype=float) / nA\n",
    "    def policy_fn(observation):\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_greedy_policy(Q):\n",
    "    \"\"\"\n",
    "    根据Q构建一个贪婪的策略\n",
    "    \n",
    "    参数:\n",
    "        Q: 一个dictionary state -> action values\n",
    "        \n",
    "    返回值:\n",
    "        返回一个函数，它的输入是状态(observation)，输出是长度为nA的numpy数值，表示每个行为的概率\n",
    "    \"\"\"\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        A = np.zeros_like(Q[state], dtype=float)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        A[best_action] = 1.0\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_control_importance_sampling(env, num_episodes, behavior_policy, discount_factor=1.0):\n",
    "    \"\"\"\n",
    "    使用加权重要性采样的Off-Policy蒙特卡罗控制\n",
    "    \n",
    "    参数:\n",
    "        env: OpenAI gym env对象\n",
    "        num_episodes: 采样的Episode次数\n",
    "        behavior_policy: 用来生成episode的行为策略。\n",
    "            它是一个函数，输入是状态，输出是一个向量，表示这个状态下每种策略的概率\n",
    "        discount_factor: 打折因子\n",
    "    \n",
    "    返回值:\n",
    "        一个tuple (Q, policy)\n",
    "        Q 是一个dictionary state -> action values\n",
    "        policy是一个函数，输入是状态，输出是一个向量，表示这个状态下每种策略的概率，这里返回的策略是一个固定的贪心的策略。\n",
    "    \"\"\"\n",
    "    \n",
    "    # action-value function.\n",
    "    # 一个dictionary：state -> action values\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # 权重的累加值，也是一个dictionary：state -> action权重累加值\n",
    "    C = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # 我们想学的策略pi，我们通过修改Q来间接修改它。\n",
    "    target_policy = create_greedy_policy(Q)\n",
    "        \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # 生成episode.\n",
    "        # 一个episode是一个数值，每个元素是一个三元组(state, action, reward) \n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            # 使用行为策略采样Action\n",
    "            probs = behavior_policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # 打折回报的累加和\n",
    "        G = 0.0\n",
    "        # 采样重要性比例 (回报的权重)\n",
    "        W = 1.0\n",
    "        # 当前episode从后往前遍历\n",
    "        for t in range(len(episode))[::-1]:\n",
    "            state, action, reward = episode[t]\n",
    "            # 计算t时刻到最后时刻的回报\n",
    "            G = discount_factor * G + reward\n",
    "            # 更新C\n",
    "            C[state][action] += W\n",
    "            # 使用增量更新公式更新Q\n",
    "            # 通过更新Q也间接的更新了目标策略pi\n",
    "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "            # 如果行为策略选择的Action不是目标策略的Action，则p(s|a)=0，我们可以退出For循环\n",
    "            if action !=  np.argmax(target_policy(state)):\n",
    "                break\n",
    "            W = W * 1./behavior_policy(state)[action]\n",
    "        \n",
    "    return Q, target_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 176000/500000."
     ]
    }
   ],
   "source": [
    "random_policy = create_random_policy(env.action_space.n)\n",
    "Q, policy = mc_control_importance_sampling(env, num_episodes=500000, behavior_policy=random_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "V = defaultdict(float)\n",
    "for state, action_values in Q.items():\n",
    "    action_value = np.max(action_values)\n",
    "    V[state] = action_value\n",
    "plotting.plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py35",
   "language": "python",
   "name": "py35"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
